# -*- coding: utf-8 -*-
"""
GRPO Training with a single GPU.
Updated reward functions.
"""

import os
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, Optional
import re
from datasets import load_dataset
import torch
from loguru import logger
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from transformers.trainer_utils import get_last_checkpoint
from transformers.integrations import is_deepspeed_zero3_enabled
from trl import GRPOConfig, GRPOTrainer, ModelConfig, TrlParser
from peft import LoraConfig, TaskType, get_peft_model
from latex2sympy2_extended import NormalizationConfig
from math_verify import LatexExtractionConfig, parse, verify
from sentence_transformers import SentenceTransformer, util
import torch.nn.functional as F


# =================== é…ç½®åŒº =====================
os.environ["TOKENIZERS_PARALLELISM"] = "FALSE"
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# å¤§æ¨¡å‹è£åˆ¤
JUDGE_MODEL_PATH = "/root/autodl-tmp/medical/MedicalGPT/models/base/Qwen2.5-3B-Instruct"
judge_tokenizer = AutoTokenizer.from_pretrained(JUDGE_MODEL_PATH)

judge_model = AutoModelForCausalLM.from_pretrained(
    JUDGE_MODEL_PATH, 
    device_map="auto", # ä¹Ÿå¯ä»¥å†™æ­» {"": 0}
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

# è¯­è¨€é¡ºæ»‘åº¦è£åˆ¤
PPL_CHECKER_PATH = "/root/autodl-tmp/medical/MedicalGPT/models/base/Qwen2.5-0.5B"
ppl_tokenizer = AutoTokenizer.from_pretrained(PPL_CHECKER_PATH)
ppl_model = AutoModelForCausalLM.from_pretrained(
    PPL_CHECKER_PATH, 
    device_map="cuda:0",
    torch_dtype=torch.bfloat16
).eval()

# å‘é‡æ¨¡å‹
VECTOR_MODEL_PATH = "/root/autodl-tmp/medical/MedicalGPT/models/text2vec_model"
semantic_judge = SentenceTransformer(VECTOR_MODEL_PATH).to("cuda")


# å‚æ•°é…ç½®ç±»
@dataclass
class ScriptArguments:
    """
    The name of the Casual LM model we wish to fine with GRPO
    """
    tokenizer_name_or_path: Optional[str] = field(
        default=None, metadata={"help": "The tokenizer for weights initialization."}
    )
    # Dataset arguments
    dataset_name: Optional[str] = field(
        default="openai/gsm8k",
        metadata={"help": "The name of the dataset to use (via the datasets library)."}
    )
    train_file_dir: Optional[str] = field(
        default=None, metadata={"help": "Directory containing training files for local datasets."}
    )
    train_samples: Optional[int] = field(default=-1, metadata={"help": "Number of samples to train on, -1 for all"})
    subset_name: Optional[str] = field(default="main",
                                       metadata={"help": "Subset name, e.g., 'default', 'main'. default is 'default'"})
    dataset_splits: Optional[str] = field(default="train", metadata={"help": "Split name"})
    preprocessing_num_workers: Optional[int] = field(default=10,
                                                     metadata={"help": "Number of workers for preprocessing"})
    # QLoRA arguments
    qlora: bool = field(default=False, metadata={"help": "Whether to use qlora"})


# =================== å·¥å…·å‡½æ•°åŒº =====================
def normalize_text(text):
    """Normalize text by removing extra whitespace, converting to lowercase."""
    if text is None:
        return ""
    # Remove extra whitespace and convert to lowercase
    text = re.sub(r'\s+', ' ', text.strip().lower())
    return text


def extract_answer(text):
    """Extract content between <answer> tags."""
    if text is None:
        return ""
    match = re.search(r'<answer>(.*?)</answer>', text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return text.strip()

def accuracy_reward(completions, answer, **kwargs):
    """å¥–åŠ±å‡½æ•°: æ ¹æ®æ¨¡å‹è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆçš„ä¸€è‡´æ€§è®¡ç®—å¥–åŠ±åˆ†æ•°"""
    # æå–æ¨¡å‹è¾“å‡ºçš„å†…å®¹ 
    # completions æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«äº†è¿™ä¸€ç»„ï¼ˆGroupï¼‰ç”Ÿæˆçš„å¤šä¸ªå€™é€‰ç­”æ¡ˆ
    # completion[0]["content"] è·å–ç¬¬ i ä¸ªç”Ÿæˆçš„å¯¹è¯æ­£æ–‡
    contents = [completion[0]["content"] for completion in completions]
    rewards = []
    # éå†æ¯ä¸€ä¸ªç”Ÿæˆçš„å›ç­”å†…å®¹å’Œå¯¹åº”çš„æ ‡å‡†ç­”æ¡ˆ (sol)
    for content, sol in zip(contents, answer):
        if '####' in sol:
            # é’ˆå¯¹ GSM8K æ•°å­¦é¢˜é›†çš„ç‰¹æ®Šå¤„ç†é€»è¾‘
            # è§£ææ ‡å‡†ç­”æ¡ˆï¼šå– #### åçš„æ•°å­—å¹¶è¿›è¡Œæ ‡å‡†åŒ–è§£æï¼ˆparseï¼‰
            gold_parsed = parse(sol.split("####", 1)[-1].strip())
            # è§£ææ¨¡å‹ç­”æ¡ˆï¼šå…ˆè°ƒç”¨ extract_answer æŠ å‡ºæ¨¡å‹åå‡ºçš„æ•°å­—ï¼Œå†è§£æ
            answer_parsed = parse(extract_answer(content))
        else:
            # é’ˆå¯¹é GSM8Kï¼ˆé€šå¸¸æ˜¯ LaTeX æˆ–é€šç”¨æ•°å­¦/åŒ»ç–—é¢˜ï¼‰çš„å¤„ç†é€»è¾‘
            # ä½¿ç”¨ LatexExtractionConfig å°è¯•ä»æ ‡å‡†ç­”æ¡ˆä¸­æå– LaTeX æ ¼å¼çš„æ•°å­¦è¡¨è¾¾å¼
            gold_parsed = parse(
                sol,
                extraction_mode="first_match",
                extraction_config=[LatexExtractionConfig()],
            )
            # è§£ææ¨¡å‹ç­”æ¡ˆï¼šè¦æ±‚æä¾›æ­£ç¡®çš„ LaTeX æ ¼å¼ï¼ˆæ— æ ¼å¼é”™è¯¯çš„è¿ç®—ç¬¦ï¼‰
            answer_parsed = parse(
                content,
                extraction_config=[
                    LatexExtractionConfig(
                        normalization_config=NormalizationConfig(
                            nits=False,
                            malformed_operators=False,  # # å…è®¸ä¸è§„èŒƒçš„è¿ç®—ç¬¦
                            basic_latex=True,   # åŸºç¡€LaTeXè¯†åˆ«
                            equations=True, # æ–¹ç¨‹è¯†åˆ«
                            boxed="all",    # å¼ºåˆ¶ä¼˜å…ˆå¯»æ‰¾ \boxed{} é‡Œçš„ç­”æ¡ˆ
                            units=True,     # è¯†åˆ«å•ä½ï¼ˆå¦‚ mg, mlï¼‰
                        ),
                        # å¯¹äºé GSM8K çš„é¢˜ç›®ï¼Œæ¨¡å‹è¾“å‡ºä¸­å¯èƒ½åŒ…å«å¤šä¸ªæ•°å­¦è¡¨è¾¾å¼ï¼ˆå¦‚å¤šä¸ªæ­¥éª¤çš„è§£ç­”ï¼‰ï¼Œæˆ‘ä»¬ä¼˜å…ˆè€ƒè™‘è¢« <answer> æ ‡ç­¾åŒ…è£¹çš„å†…å®¹ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™æŒ‰ç…§é¡ºåºæå–ç¬¬ä¸€ä¸ªç¬¦åˆæ¡ä»¶çš„è¡¨è¾¾å¼ä½œä¸ºç­”æ¡ˆè¿›è¡ŒéªŒè¯
                        boxed_match_priority=0,
                        try_extract_without_anchor=False,
                    )
                ],
                extraction_mode="first_match",
            )
        # åˆ¤å®šé˜¶æ®µï¼šè°ƒç”¨ math-verify åº“è¿›è¡Œâ€œè¯­ä¹‰å¯¹é½â€åˆ¤å®š
        try:
            # verifyå‡½æ•°èƒ½åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦ä¸€è‡´ï¼Œå¹¶è¿”å›ä¸€ä¸ªå¸ƒå°”å€¼ï¼ˆTrue/Falseï¼‰ã€‚æˆ‘ä»¬å°†å…¶è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼ˆ1.0/0.0ï¼‰ä½œä¸ºå¥–åŠ±åˆ†æ•°
            reward = float(verify(answer_parsed, gold_parsed))
        except Exception as e:
            logger.warning(f"Error in verification: {e}")
            reward = 0.0
        # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œæ–¹ä¾¿åœ¨åå°çœ‹æ¨¡å‹åˆ°åº•ç­”å¯¹äº†æ²¡
        logger.debug(f"predict_answer: {content}, \nground_truth: {sol}, \n"
                     f"answer_parsed: {answer_parsed}, gold_parsed: {gold_parsed}, reward: {reward}\n\n")
        rewards.append(reward)
    # æ±‡æ€»è¿™ä¸€ç»„ï¼ˆGroupï¼‰æ‰€æœ‰æ ·æœ¬çš„å¥–åŠ±å€¼å¹¶è¿”å›
    logger.debug(f'accuracy rewards: {rewards}')
    return rewards


def format_reward(completions, **kwargs):
    """å¥–åŠ±å‡½æ•°: ä¿è¯æ ¼å¼æ­£ç¡® (CoT)."""
    if os.environ.get("LOCAL_RANK", "0") == "0":
        print(f"\n[SAMPLE OUTPUT]: {completions[0][0]['content'][:200]}...", flush=True)
    
    pattern = r"<think>.*?</think><answer>.*?</answer>$"
    completion_contents = [completion[0]["content"] for completion in completions]
    matches = [re.match(pattern, content) for content in completion_contents]
    # è®¡ç®—å¹¶æ‰“å°å¥–åŠ±åˆ†æ•°
    rewards = [2.0 if match else 0.0 for match in matches]
    logger.debug(f'format rewards: {rewards}')
    return rewards


def semantic_reward(completions, answer, **kwargs) -> list[float]:
    """å¥–åŠ±å‡½æ•°: æ¨¡å‹å›ç­”å’Œæ ‡å‡†ç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
    # è·å–å›ç­”
    responses = [extract_answer(c[0]["content"]) for c in completions]
    
    # å‘é‡åŒ–å½“å‰ Batch çš„æ‰€æœ‰å›ç­”å’Œæ ‡å‡†ç­”æ¡ˆ
    pred_embeddings = semantic_judge.encode(responses, convert_to_tensor=True)
    gold_embeddings = semantic_judge.encode(answer, convert_to_tensor=True)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_scores = util.cos_sim(pred_embeddings, gold_embeddings)
    # æå–å¯¹è§’çº¿ä¸Šçš„åˆ†å€¼ï¼ˆå³å¯¹åº”çš„æ ·æœ¬å¯¹ï¼‰
    scores = torch.diagonal(cosine_scores).tolist()
    logger.debug(f'semantic rewards: {scores}')
    return [float(s) for s in scores]


def anti_repetition_reward(completions, **kwargs) -> list[float]:
    """å¥–åŠ±å‡½æ•°: æƒ©ç½šç®€å•çš„é‡å¤å¥å­/æ®µè½"""
    rewards = []
    for completion in completions:
        content = completion[0]["content"]
        # ä»¥å¥å­ä¸ºå•ä½ï¼Œåˆ¤æ–­æ˜¯å¦å­˜åœ¨è¿‡åº¦é‡å¤
        sentences = [s.strip() for s in re.split(r"[ã€‚ï¼ï¼Ÿ!?ï¼›;\n]+", content) if s.strip()]
        if len(sentences) >= 3:
            unique_sentences = set(sentences)
            max_repeat_ratio = max(sentences.count(s) for s in unique_sentences) / len(sentences)
            # è‹¥æŸä¸ªå¥å­å æ¯”è¿‡é«˜ä¸”é‡å¤å‡ºç°ï¼Œè§†ä¸ºåˆ·é‡å¤
            if max_repeat_ratio > 0.4 and max(sentences.count(s) for s in unique_sentences) >= 2:
                rewards.append(-1.0)
            else:
                rewards.append(0.0)
        else:
            rewards.append(0.0)
    logger.debug(f'anti repetition rewards: {rewards}')
    return rewards


def llm_judge_reward(completions, answer, **kwargs) -> list[float]:
    """å¥–åŠ±å‡½æ•°: ä½¿ç”¨å¤§æ¨¡å‹ç»™å›ç­”è´¨é‡æ‰“åˆ†"""
    # æå–æ¨¡å‹å›ç­”
    responses = [c[0]["content"] for c in completions]
    prompts = kwargs.get("prompts", [""] * len(responses)) # è·å–åŸå§‹æé—®
    rewards = []
    
    for i, (res, gold) in enumerate(zip(responses, answer)):
        # å…ˆæŸ¥çœ‹åŸæ–‡
        if os.environ.get("LOCAL_RANK", "0") == "0" and i == 0:
            print(f"\n --- [JUDGE DEBUG] --- \n", flush=True)
            print(f"ã€ğŸ˜† å­¦ç”ŸåŸè¯ã€‘: \n {res}...", flush=True)
        
        # æ„é€ æ ‡å‡†chat_template
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åèµ„æ·±çš„åŒ»å­¦æ•™æˆã€‚è¯·è¯„ä»·ä¸‹æ–¹å­¦ç”Ÿå¯¹åŒ»ç–—é—®é¢˜çš„å›ç­”ã€‚"},
            {"role": "user", "content": f"""
             è¯·ä»å›ç­”ä¸“ä¸šæ€§ã€é€»è¾‘ä¸¥å¯†æ€§ç­‰ç»´åº¦è¯„ä»·å­¦ç”Ÿçš„åŒ»ç–—å›ç­”ã€‚
             è¦æ±‚ï¼šå…ˆç›´æ¥ç»™å‡ºåˆ†æ•°ï¼Œå†ç®€è¦è¯´æ˜ç†ç”±ï¼Œæ‰“åˆ†é‡ç¨‹ï¼š0-10åˆ†ã€‚
             æ‰“åˆ†è¦æ±‚ï¼š
                1. å¦‚æœå›ç­”é€»è¾‘é”™è¯¯æˆ–æœ‰è¯¯å¯¼ï¼Œç»™ 0-3 åˆ†ã€‚
                2. å¦‚æœå›ç­”åŸºæœ¬æ­£ç¡®ä½†æœ‰ç‘•ç–µï¼Œç»™ 4-7 åˆ†ã€‚
                3. å¦‚æœå›ç­”å®Œç¾ä¸”é€»è¾‘æ¸…æ™°ï¼Œç»™ 8-10 åˆ†ã€‚
             åŠ¡å¿…ä»¥ [[åˆ†å€¼]] çš„æ ¼å¼ç»™å‡ºæ€»åˆ†ã€‚
             ã€å‚è€ƒæ ‡å‡†ç­”æ¡ˆã€‘ï¼š{gold}\n
             ã€å­¦ç”Ÿç”Ÿæˆçš„å›ç­”ã€‘ï¼š{res}\n"""}
        ]
        input_ids = judge_tokenizer.apply_chat_template(
            messages, 
            tokenize=True, 
            add_generation_prompt=True, 
            return_tensors="pt"
        ).to(judge_model.device)
        
        with torch.no_grad():
            outputs = judge_model.generate(
                **input_ids,    # ** è§£åŒ…å­—å…¸ä¸ºå…³é”®å­—å‚æ•°
                max_new_tokens=256, 
                do_sample=False,  # è¿›ä¸€æ­¥å¼ºåˆ¶ç¡®å®šæ€§
                pad_token_id=judge_tokenizer.pad_token_id
            )
            # åªè§£ç æ¨¡å‹æ–°åå‡ºæ¥çš„éƒ¨åˆ†
            prompt_len = input_ids["input_ids"].shape[1]
            new_tokens = outputs[0][prompt_len:]
            judge_response = judge_tokenizer.decode(new_tokens, skip_special_tokens=True)

        # æ­£åˆ™æå–åˆ†æ•°
        def flexible_score_parser(text):
            """æ­£åˆ™åŒ–æå–åˆ†æ•°, åŒ…å«ä¼˜å…ˆçº§"""
            # 1. ä¼˜å…ˆçº§æœ€é«˜ï¼šæ‰¾æ ‡å‡†çš„ [[8]]
            match = re.search(r"\[\[(\d+\.?\d*)\]\]", text)
            if match: return float(match.group(1))
            
            # 2. ä¼˜å…ˆçº§ä¸­ç­‰ï¼šæ‰¾ç±»ä¼¼ **åˆ†æ•°ï¼š2åˆ†** æˆ– åˆ†æ•°: 2
            match = re.search(r"åˆ†æ•°[:ï¼š]\s*(\d+\.?\d*)", text)
            if match: return float(match.group(1))
            
            # 3. ä¼˜å…ˆçº§æœ€ä½ï¼šæ‰¾å­—ç¬¦ä¸²é‡Œçš„ç¬¬ä¸€ä¸ªæ•°å­—
            match = re.search(r"(\d+\.?\d*)", text)
            if match: return float(match.group(1))
            return 0.0

        score = flexible_score_parser(judge_response)
        rewards.append(score / 10.0)    # æ ‡å‡†åŒ–

        # æŸ¥çœ‹è£åˆ¤åŸæ–‡
        if os.environ.get("LOCAL_RANK", "0") == "0" and i == 0:
            print(f"ã€ğŸ¤¨ è£åˆ¤åŸè¯ã€‘:\n {judge_response}", flush=True)
    logger.debug(f'llm judge rewards: {rewards}')
    return rewards



def ppl_penalty_reward(completions, **kwargs) -> list[float]:
    """å¥–åŠ±å‡½æ•°: è¯­è¨€é¡ºæ»‘åº¦æƒ©ç½š (PPL Penalty)"""
    responses = [c[0]["content"] for c in completions]
    rewards = []

    for text in responses:
        if len(text.strip()) < 5: # å¤ªçŸ­çš„æ•°æ®ä¸æµ‹ï¼Œç›´æ¥ç»™ 0
            rewards.append(0.0)
            continue

        # 1. åˆ†è¯
        inputs = ppl_tokenizer(text, return_tensors="pt").to(ppl_model.device)
        input_ids = inputs["input_ids"]
        
        with torch.no_grad():
            # 2. è·å–æ¨¡å‹çš„ Logits
            outputs = ppl_model(input_ids, labels=input_ids)
            # è¿™é‡Œçš„ loss å®é™…ä¸Šå°±æ˜¯å¹³å‡ Negative Log-Likelihood (NLL)
            nll = outputs.loss.item() 
        
        # 3. æ˜ å°„ä¸ºæƒ©ç½šé¡¹
        # æ­£å¸¸äººç±»è¯­è¨€çš„ NLL é€šå¸¸åœ¨ 1.0 - 4.0 ä¹‹é—´
        # å¦‚æœ nll > 5.0ï¼Œè¯´æ˜æ¨¡å‹å¼€å§‹ä¹±è¯´è¯äº†ï¼Œæˆ‘ä»¬å¼€å§‹æ‰£åˆ†
        if nll > 5.0:
            # æƒ©ç½šå…¬å¼ï¼šè¶…è¿‡ 5 çš„éƒ¨åˆ†ï¼Œæ¯å¤š 1 ç‚¹æ‰£ 0.5 åˆ†
            penalty = -0.5 * (nll - 5.0)
            rewards.append(max(penalty, -2.0)) # è®¾ç½®æ‰£åˆ†ä¸‹é™ï¼Œé˜²æ­¢ Loss çˆ†ç‚¸
        else:
            rewards.append(0.0) # è¯´äººè¯ï¼Œä¸æ‰£åˆ†
    logger.debug(f'ppl penalty rewards: {rewards}')
    return rewards


# ä¿®æ”¹åŒ»å­¦ä»»åŠ¡é€‚é…çš„system prompt
SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ã€‚ç”¨æˆ·ä¼šå‘ä½ å’¨è¯¢åŒ»å­¦é—®é¢˜ï¼Œè¯·ä½ é€šè¿‡æ·±åº¦æ€è€ƒåç»™å‡ºå‡†ç¡®çš„è§£ç­”ã€‚\n"
    "ã€æ ¸å¿ƒè¦æ±‚ã€‘ï¼š\n"
    "1. ä½ çš„å›ç­”å¿…é¡»åŒ…å«â€˜æ€ç»´é“¾æ¨ç†â€™å’Œâ€˜æœ€ç»ˆç­”æ¡ˆâ€™ä¸¤éƒ¨åˆ†ã€‚\n"
    "2. æ¨ç†è¿‡ç¨‹è¯·æ”¾åœ¨ <think> å’Œ </think> æ ‡ç­¾ä¹‹é—´ï¼Œè¯¦ç»†åˆ†æç—…ç†ã€é€»è¾‘å’Œé‰´åˆ«è¯Šæ–­ã€‚\n"
    "3. æœ€ç»ˆç­”æ¡ˆè¯·æ”¾åœ¨ <answer> å’Œ </answer> æ ‡ç­¾ä¹‹é—´ï¼Œç»™å‡ºç²¾ç‚¼ã€ä¸“ä¸šçš„åŒ»å­¦å»ºè®®ã€‚\n"
    "æ ¼å¼ç¤ºä¾‹ï¼š<think> åœ¨è¿™é‡Œè¿›è¡Œæ·±å…¥æ¨ç†... </think><answer> æœ€ç»ˆç»“è®ºå’Œå»ºè®®... </answer>"
)

# è·å–æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¦‚æœå­˜åœ¨çš„è¯ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒä¸­æ–­åæ¢å¤è®­ç»ƒ
def get_checkpoint(training_args: GRPOConfig):
    last_checkpoint = None
    if os.path.isdir(training_args.output_dir):
        last_checkpoint = get_last_checkpoint(training_args.output_dir)
    return last_checkpoint

# æ¨¡å‹æœ‰æˆç™¾ä¸Šåƒå±‚ï¼Œåˆ°åº•è¯¥æŠŠ LoRA çš„â€œè¡¥ä¸â€è´´åœ¨å“ªé‡Œ
def find_all_linear_names(peft_model, int4=False, int8=False):
    """å¯»æ‰¾æ‰€æœ‰å¯æ³¨å…¥ LoRA çš„çº¿æ€§å±‚åç§°"""
    # è®¾ç½®ç›®æ ‡å±‚çš„åˆå§‹ç±»å‹ä¸º PyTorch å®˜æ–¹çš„æ ‡å‡†çº¿æ€§å±‚
    cls = torch.nn.Linear
    # å¦‚æœå¼€å¯äº†é‡åŒ–ï¼ˆ4ä½æˆ–8ä½ï¼‰ï¼Œéœ€è¦åˆ‡æ¢æŸ¥æ‰¾çš„ç›®æ ‡ç±»å‹
    if int4 or int8:
        # å¯¼å…¥ bitsandbytes åº“ï¼Œå®ƒæ˜¯å®ç°é‡åŒ–å¾®è°ƒçš„åº•å±‚æ ¸å¿ƒ
        import bitsandbytes as bnb
        if int4:
            cls = bnb.nn.Linear4bit
        elif int8:
            cls = bnb.nn.Linear8bitLt
    # åˆ›å»ºä¸€ä¸ªé›†åˆï¼Œç”¨æ¥å­˜å‚¨å‘ç°çš„çº¿æ€§å±‚çŸ­åç§°ï¼Œset å…·æœ‰è‡ªåŠ¨å»é‡åŠŸèƒ½
    lora_module_names = set()
    # éå†æ¨¡å‹ä¸­çš„æ‰€æœ‰å­æ¨¡å—
    # name æ˜¯å®Œæ•´è·¯å¾„ï¼ˆå¦‚ model.layers.0.mlp.gate_projï¼‰ï¼Œmodule æ˜¯å¯¹åº”çš„å±‚å¯¹è±¡
    for name, module in peft_model.named_modules():
        # æ£€æŸ¥å½“å‰çš„è¿™ä¸ªå±‚æ˜¯ä¸æ˜¯æˆ‘ä»¬åˆšæ‰å®šä¹‰çš„çº¿æ€§å±‚ç±»å‹
        if isinstance(module, cls):
            # lm_head æ˜¯é¢„æµ‹å•è¯çš„æœ€åä¸€å±‚ï¼Œé€šå¸¸ä¸å»ºè®®åŠ  LoRAï¼Œä»¥ä¿æŒè¾“å‡ºç¨³å®š
            if 'lm_head' in name:
                continue
            # æœ‰çš„æ¨¡å‹æŠŠè¾“å‡ºå±‚å« output_layerï¼ŒåŒæ ·è·³è¿‡
            if 'output_layer' in name:
                continue
            # æå–å±‚çš„çŸ­åç§° 
            # å°† 'model.layers.0.self_attn.q_proj' æŒ‰ '.' åˆ‡åˆ†
            names = name.split('.')
            # å¦‚æœåå­—åªæœ‰ä¸€çº§å°±å– names[0]ï¼Œå¦åˆ™å–æœ€åä¸€éƒ¨åˆ† names[-1]ï¼ˆå¦‚ 'q_proj'
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    return sorted(lora_module_names)


def grpo_train(model_args: ModelConfig, script_args: ScriptArguments, training_args: GRPOConfig):
    '''å®Œæ•´çš„GRPOè®­ç»ƒæµç¨‹, åŒ…æ‹¬DDPã€å¤šå¡ã€é‡åŒ–ã€LoRA'''
    # åˆ†å¸ƒå¼è®­ç»ƒåˆå§‹åŒ–
    is_main_process = training_args.local_rank in [-1, 0]

    # åˆ¤æ–­æ˜¯å¦ä¸»è¿›ç¨‹ï¼Œä»…ä¸»è¿›ç¨‹è¾“å‡ºæ—¥å¿—
    if is_main_process:
        logger.warning(
            f"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}"
            + f" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}"
        )
        logger.info(f"Model parameters {model_args}")
        logger.info(f"Script parameters {script_args}")
        logger.info(f"Training parameters {training_args}")

    # åŠ è½½tokenizerï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ tokenizerï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨æ¨¡å‹è‡ªå¸¦çš„ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        (
            script_args.tokenizer_name_or_path
            if script_args.tokenizer_name_or_path
            else model_args.model_name_or_path
        ),
        revision=model_args.model_revision,
        padding_side="left",
        trust_remote_code=model_args.trust_remote_code,
    )
    # é…ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # åŠ è½½æ•°æ®é›†ï¼Œä¼˜å…ˆä»æœ¬åœ°ç›®å½•åŠ è½½ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» HuggingFace hub åŠ è½½
    if script_args.train_file_dir and os.path.exists(script_args.train_file_dir):
        # ä»æœ¬åœ°ç›®å½•åŠ è½½æ•°æ®é›†
        dataset = load_dataset("json", data_dir=script_args.train_file_dir, split="train")
    else:
        # ä» HuggingFace hub åŠ è½½æ•°æ®é›†
        dataset = load_dataset(script_args.dataset_name, script_args.subset_name, split=script_args.dataset_splits)
    # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è®­ç»ƒæ ·æœ¬æ•°é‡ï¼Œåˆ™éšæœºæ‰“ä¹±æ•°æ®é›†å¹¶é€‰å–å‰ N ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
    if script_args.train_samples > 0:
        dataset = dataset.shuffle(seed=42).select(range(script_args.train_samples))

    # å‡†å¤‡æ•°æ®é›†
    with training_args.main_process_first(desc="Dataset preparation"):
        def extract_to_grpo(example):
            user_question = example.get("question", "")
            gold_answer = example.get("response_chosen", "")
            return {
                'prompt': [
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': user_question}
                ],
                'answer': gold_answer
            }

        dataset = dataset.map(
            extract_to_grpo,
            num_proc=script_args.preprocessing_num_workers,
            remove_columns=dataset.column_names,
            desc="Processing dataset" if is_main_process else None,
        )

    # åˆ’åˆ†æ•°æ®é›†
    train_test_split = dataset.train_test_split(test_size=0.1)
    train_dataset = train_test_split["train"]
    test_dataset = train_test_split["test"]

    if is_main_process:
        logger.info("*** Initializing model kwargs ***")

    # æ¨¡å‹åˆå§‹åŒ–å‚æ•°è®¾ç½®
    # model_argsæ‰¾ä¸åˆ°torch_dtype ç›´æ¥ç”¨training_argsçš„bf16å’Œfp16æ¥è‡ªåŠ¨è¯†åˆ«torch_dtype
    if training_args.bf16:
        torch_dtype = torch.bfloat16
    elif training_args.fp16:
        torch_dtype = torch.float16
    else:
        torch_dtype = torch.float32
    
    logger.info(f"æ ¹æ®è®­ç»ƒé…ç½®ï¼Œè‡ªåŠ¨è¯†åˆ« torch_dtype ä¸º: {torch_dtype}")

    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
    world_size = int(os.environ.get("WORLD_SIZE", "1"))
    ddp = world_size != 1

    # æ£€æŸ¥ QLoRA å…¼å®¹æ€§
    if script_args.qlora and is_deepspeed_zero3_enabled():
        logger.warning("ZeRO3 are both currently incompatible with QLoRA.")

    # æ£€æŸ¥é‡åŒ–è®¾ç½®ï¼Œ4bit å’Œ 8bit ä¸èƒ½åŒæ—¶å¼€å¯
    if model_args.load_in_4bit and model_args.load_in_8bit:
        raise ValueError("Error, load_in_4bit and load_in_8bit cannot be set at the same time")

    # è®¾ç½®é‡åŒ–é…ç½®
    quantization_config = None
    if script_args.qlora and (model_args.load_in_4bit or model_args.load_in_8bit):
        if is_main_process:
            logger.info(
                f"Quantizing model, load_in_4bit: {model_args.load_in_4bit}, load_in_8bit: {model_args.load_in_8bit}")
        if is_deepspeed_zero3_enabled():
            raise ValueError("DeepSpeed ZeRO-3 is incompatible with quantization.")

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=model_args.load_in_4bit,
            load_in_8bit=model_args.load_in_8bit,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch_dtype,
        )
    elif model_args.load_in_4bit or model_args.load_in_8bit:
        # Support quantization even without qlora flag
        if is_main_process:
            logger.info(
                f"Quantizing model, load_in_4bit: {model_args.load_in_4bit}, load_in_8bit: {model_args.load_in_8bit}")
        if is_deepspeed_zero3_enabled():
            raise ValueError("DeepSpeed ZeRO-3 is incompatible with quantization.")

        quantization_config = BitsAndBytesConfig(
            load_in_4bit=model_args.load_in_4bit,
            load_in_8bit=model_args.load_in_8bit,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch_dtype,
        )

    model_kwargs = dict(
        revision=model_args.model_revision,
        trust_remote_code=model_args.trust_remote_code,
        attn_implementation=model_args.attn_implementation,
        torch_dtype=torch_dtype,
        low_cpu_mem_usage=(not is_deepspeed_zero3_enabled()),
        quantization_config=quantization_config,
    )
    
    # åˆ†å¸ƒå¼è®­ç»ƒå’Œå¤šå¡è®­ç»ƒè®¾ç½®
    num_gpus = torch.cuda.device_count()
    if ddp:
        device_map = {"": int(os.environ.get("LOCAL_RANK", "0"))}
        model_kwargs["device_map"] = device_map
        # Ensure gradient_accumulation_steps is at least 1 after division
        training_args.gradient_accumulation_steps = max(training_args.gradient_accumulation_steps // world_size, 1)
    elif num_gpus > 1:
        max_memory = {}
        for i in range(num_gpus):
            gpu_props = torch.cuda.get_device_properties(i)
            total_mem = gpu_props.total_memory
            # é¢„ç•™20%å†…å­˜ç»™è®­ç»ƒæ—¶çš„æ¢¯åº¦ã€ä¼˜åŒ–å™¨çŠ¶æ€ç­‰
            usable_mem = int(total_mem * 0.8)
            max_memory[i] = f"{usable_mem // (1024 ** 3)}GiB"
        model_kwargs["max_memory"] = max_memory
        model_kwargs["device_map"] = "auto"
    else:
        model_kwargs["device_map"] = "auto"

    if is_main_process:
        logger.info(f"Using {num_gpus} GPUs")
        logger.info(f"model_kwargs={model_kwargs}")

    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        **model_kwargs,
    )

    if is_main_process and hasattr(model, 'hf_device_map'):
        logger.info(f"Model Device Map: {model.hf_device_map.items()}")
    elif is_main_process and num_gpus > 1:
        logger.info("Model Device Map:")
        for name, param in model.named_parameters():
            if hasattr(param, 'device'):
                logger.info(f"  {name}: {param.device}")
                break

    # é…ç½®LoRA
    if model_args.use_peft:
        if is_main_process:
            logger.info("Fine-tuning method: LoRA(PEFT)")
        if training_args.gradient_checkpointing:
            logger.warning("Gradient checkpointing is enabled. It may cause issues with LoRA, setting it to False.")
            training_args.gradient_checkpointing = False
        target_modules = model_args.lora_target_modules if model_args.lora_target_modules else None
        if target_modules == 'all' or (target_modules and 'all' in target_modules):
            target_modules = find_all_linear_names(model, int4=model_args.load_in_4bit, int8=model_args.load_in_8bit)
        if is_main_process:
            logger.info(f"Peft target_modules: {target_modules}, lora rank: {model_args.lora_r}, ")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            target_modules=target_modules,
            inference_mode=False,
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            lora_dropout=model_args.lora_dropout,
        )
        model = get_peft_model(model, peft_config)
        # Fixed FP16 ValueError for quantized models
        for param in filter(lambda p: p.requires_grad, model.parameters()):
            param.data = param.data.to(torch.float32)
        model.print_trainable_parameters()
    else:
        if is_main_process:
            logger.info("Fine-tuning method: Full parameters training")

    if training_args.gradient_checkpointing and getattr(model, "supports_gradient_checkpointing", False):
        model.gradient_checkpointing_enable()
        model.config.use_cache = False
        logger.info("Gradient checkpointing enabled.")
    else:
        model.config.use_cache = True
        logger.info("Gradient checkpointing disabled.")

    # åˆå§‹åŒ–GRPOTrainer
    trainer = GRPOTrainer(
        model=model,
        processing_class=tokenizer,
        reward_funcs=[
            llm_judge_reward,
            ppl_penalty_reward,
            format_reward,
            semantic_reward
        ],
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset if training_args.eval_strategy != "no" else None,
    )
    logger.info("*** GRPO Trainer initialized ***")
    logger.debug(f"Trainer: {trainer}")

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¹‹å‰çš„æ£€æŸ¥ç‚¹ï¼Œå¦‚æœå­˜åœ¨åˆ™ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    last_checkpoint = get_checkpoint(training_args)
    if last_checkpoint is not None and training_args.resume_from_checkpoint is None:
        if is_main_process:
            logger.info(f"Checkpoint detected, resuming training at {last_checkpoint}.")

    if is_main_process:
        logger.info(
            f'*** Starting training {datetime.now().strftime("%Y-%m-%d %H:%M:%S")} for '
            f'{training_args.num_train_epochs} epochs ***'
        )

    train_result = trainer.train(resume_from_checkpoint=last_checkpoint)

    # è®­ç»ƒå®Œæˆåï¼Œåªæœ‰ä¸»è¿›ç¨‹è´Ÿè´£è®°å½•å’Œä¿å­˜è®­ç»ƒç»“æœï¼Œä»¥é¿å…å¤šè¿›ç¨‹é‡å¤å†™å…¥æ—¥å¿—å’Œæ¨¡å‹æ–‡ä»¶
    if is_main_process:
        metrics = train_result.metrics
        metrics["train_samples"] = len(train_dataset)
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
        logger.info("*** Training complete ***")
        logger.info("*** Save model ***")

    # ä¿å­˜æ¨¡å‹
    trainer.model.config.use_cache = True
    if is_main_process:
        trainer.save_model(training_args.output_dir)
        logger.info(f"Model saved to {training_args.output_dir}")

    training_args.distributed_state.wait_for_everyone()

    if is_main_process:
        tokenizer.save_pretrained(training_args.output_dir)
        logger.info(f"Tokenizer saved to {training_args.output_dir}")

        # Create model card and save config
        kwargs = {
            "dataset_name": script_args.dataset_name,
            "tags": ["r1", "grpo"],
        }
        trainer.create_model_card(**kwargs)
        trainer.model.config.use_cache = True
        trainer.model.config.save_pretrained(training_args.output_dir)

    if is_main_process:
        logger.info("*** Training complete! ***")


def main():
    parser = TrlParser((ModelConfig, ScriptArguments, GRPOConfig))
    model_args, script_args, training_args = parser.parse_args_and_config()

    # Run the main training loop
    grpo_train(model_args, script_args, training_args)


if __name__ == "__main__":
    main()
