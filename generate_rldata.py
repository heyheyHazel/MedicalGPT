'''
è°ƒç”¨DeepSeek-R1æ¨¡å‹ç”ŸæˆåŒ»ç–—æ¨ç†å¯¹é½æ•°æ®ï¼ˆChosen/Rejctedï¼‰ï¼Œå¹¶æ”¯æŒæ–­ç‚¹ç»­è·‘å’Œæ€§èƒ½ä¼˜åŒ–ã€‚
è¾“å…¥ï¼šåŸå§‹ç—…å†æé—®ï¼ˆJSONLï¼Œæ¯è¡Œä¸€ä¸ªè®°å½•ï¼ŒåŒ…å« "question" å­—æ®µï¼‰
è¾“å‡ºï¼šåŒ…å«é‡æ„ç—…å†ã€ä¸“å®¶å›ç­”ï¼ˆChosenï¼‰å’Œå®ä¹ ç”Ÿå›ç­”ï¼ˆRejectedï¼‰çš„ JSONL æ–‡ä»¶ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{
    "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œå¿…é¡»é€šè¿‡æ·±åº¦é€»è¾‘æ¨ç†è¾…åŠ©è¯Šæ–­ã€‚",
    "history": [],
    "question": "é‡æ„åçš„ç—…å†æé—®",
    "response_chosen": "<thought>æ¨¡å‹çš„æ€ç»´é“¾</thought>\nä¸“å®¶é£æ ¼çš„å›ç­”",
    "response_rejected": "å®ä¹ ç”Ÿé£æ ¼çš„å›ç­”",
    "metadata": {
        "original_question": "åŸå§‹ç—…å†æé—®",
        "latency": 12.34
    }
}
'''
import json
import hashlib
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Any, Dict, Optional, Tuple

from openai import APIStatusError, OpenAI, OpenAIError
from tqdm import tqdm
import os


# ====== é…ç½®åŒº ======
# æå‰é…ç½®ç¯å¢ƒå˜é‡ï¼šexport OPENAI_API_KEY="sk-xxx"
api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com")
model = os.getenv("OPENAI_MODEL", "deepseek-chat")
stream = os.getenv("OPENAI_STREAM", "1").lower() in {"1", "true", "yes"}
INPUT_PATH = "data/reward/train/train.jsonl"
OUTPUT_PATH = "data/preference/preference_v1.jsonl" # è¾“å‡ºæ¸…æ´—åçš„æ¨ç†å¯¹é½æ•°æ®

# æ€§èƒ½ç›¸å…³é…ç½®ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
# - RL_GEN_MODE: two_call(é»˜è®¤) | three_call(åŸå§‹é€»è¾‘)
# - MAX_WORKERS: å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé€‚å½“è°ƒå¤§å¯æ˜¾è‘—æé€Ÿï¼Œä½†å¯èƒ½è§¦å‘é™æµï¼‰
# - OPENAI_TIMEOUT: è¯·æ±‚è¶…æ—¶ç§’æ•°ï¼ˆé¿å…é•¿æ—¶é—´å¡æ­»ï¼‰
# - RESUME: 1 è¡¨ç¤ºæ ¹æ®è¾“å‡ºæ–‡ä»¶è¡Œæ•°è·³è¿‡å·²å®Œæˆçš„è¾“å…¥è¡Œï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰
RL_GEN_MODE = os.getenv("RL_GEN_MODE", "two_call").lower()
MAX_WORKERS = int(os.getenv("MAX_WORKERS", "2"))
OPENAI_TIMEOUT = float(os.getenv("OPENAI_TIMEOUT", "180"))
RESUME = os.getenv("RESUME", "1").lower() in {"1", "true", "yes"}


# åˆå§‹åŒ– deepseek/openai compatible å®¢æˆ·ç«¯
client = OpenAI(api_key=api_key, base_url=base_url, timeout=OPENAI_TIMEOUT)

# é‡æ„ç—…å†çš„Prompt æ ¼å¼åŒ–ç—…äººç—…å²ç‰¹å¾
PROMPT_RECONSTRUCT = "ä½ æ˜¯ä¸€åç—…å†å½•å…¥å‘˜ã€‚è¯·å°†ä»¥ä¸‹æ‚£è€…çš„ä¹±åºæé—®é‡æ„ä¸ºæ ‡å‡†çš„ä¸´åºŠç—…å†æ ¼å¼ï¼ˆåŒ…æ‹¬ï¼šæ€§åˆ«ã€å¹´é¾„ã€ä¸»è¯‰ã€ç—…å²ç®€è¿°ã€æ ¸å¿ƒé—®é¢˜ï¼‰ï¼Œè¦æ±‚ç”¨è¯ä¸“ä¸šã€ç²¾ç®€ï¼Œè‹¥æœ‰æœªæåŠåˆ°çš„åˆ™æ ‡è®°ä¸ºâ€œæœªçŸ¥â€ã€‚"

# chosen/rejected çš„ Prompt æ¨¡æ¿, ä¸“å®¶ vs å®ä¹ ç”Ÿ
PROMPT_EXPERT = (
    "ä½ æ˜¯ä¸€åç»éªŒä¸°å¯Œçš„ä¸´åºŠä¸»ä»»ã€‚ç°åœ¨ä½ æ­£åœ¨è¯Šå®¤é¢å¯¹ä¸€ä½ç„¦è™‘çš„æ‚£è€…ã€‚\n"
    "è¦æ±‚ï¼š\n"
    "1. è¯­æ°”è¦äº²åˆ‡è‡ªç„¶ï¼ŒåƒåŒ»ç”ŸæŸ¥æˆ¿è¯´è¯ï¼Œå¤šç”¨â€˜æ‚¨â€™ï¼Œé¿å…AIå‘³ã€‚\n"
    "2. ç›´æ¥ç»™å‡ºè¯¦ç»†çš„è¯Šæ–­å†…å®¹å’Œ3-5æ¡æœ€å…³é”®çš„è¡ŒåŠ¨å»ºè®®ï¼Œæ€»å­—æ•°ä¸¥æ§åœ¨500å­—ä»¥å†…ã€‚\n"
    "3. ä¸è¦åœ¨å›å¤é‡Œå†™â€˜å†…éƒ¨æ€ç»´â€™ã€â€˜åˆ†æâ€™ç­‰æ ‡é¢˜ï¼Œç›´æ¥å¯¹è¯ã€‚"
)

# 3. å®ä¹ ç”Ÿ (Rejected)ï¼šç”Ÿç¡¬æ­»æ¿ã€åªä¼šèƒŒä¹¦ã€å¿½ç•¥æƒ…æ„Ÿ
PROMPT_INTERN = (
    "ä½ æ˜¯ä¸€ååˆšæ¯•ä¸šã€åªæ‡‚èƒŒä¹¦çš„å®ä¹ åŒ»ç”Ÿã€‚ä½ å¯¹æ‚£è€…ç¼ºå°‘åŒç†å¿ƒï¼Œè¯­æ°”ç”Ÿç¡¬ã€‚\n"
    "è¦æ±‚ï¼š\n"
    "1. è¯­æ°”æœºæ¢°ï¼Œåªä¼šç½—åˆ—åŒ»å­¦åè¯ï¼Œä¸æ“…é•¿å®‰æ…°æ‚£è€…ã€‚\n"
    "2. ç»™å‡ºå¤§é‡ä¸åˆ†ä¸»æ¬¡çš„æ£€æŸ¥å»ºè®®ï¼Œè®©æ‚£è€…æ„Ÿåˆ°æ›´åŠ å›°æƒ‘å’Œè´Ÿæ‹…ã€‚\n"
    "3. å›ç­”è¦ç®€çŸ­ä½†æ•·è¡ï¼Œç»™äººä¸€ç§â€˜æˆ‘åœ¨åº”ä»˜å·¥ä½œâ€™çš„æ„Ÿè§‰ã€‚"
)


def _extract_json(text: str) -> Optional[Dict[str, Any]]:
    if not text:
        return None
    text = text.strip()
    try:
        return json.loads(text)
    except Exception:
        pass

    # å®¹é”™ï¼šæ¨¡å‹æœ‰æ—¶ä¼šåœ¨ JSON å‰ååŠ è¯´æ˜æ–‡å­—
    match = re.search(r"\{[\s\S]*\}", text)
    if not match:
        return None
    try:
        return json.loads(match.group(0))
    except Exception:
        return None


# ====== é‡æ„ç—…å†prompt ======
def call_ds_chat(prompt, model="deepseek-chat", temperature=0.7):
    """é€šç”¨è°ƒç”¨å‡½æ•°"""
    for _ in range(3): # ç®€å•é‡è¯•æœºåˆ¶
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
            )
            return response.choices[0].message.content
        except (APIStatusError, OpenAIError) as e:
            print(f"APIè¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­... {e}")
            time.sleep(2)
    return None

def call_ds_reasoner(system_prompt, user_q, temperature=0.7):
    """è°ƒç”¨ R1 æ¨ç†æ¨¡å‹ï¼Œæ•è·æ€ç»´é“¾"""
    for _ in range(3):
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_q}
                ],
                temperature=temperature
            )
            # ç»„åˆ <thought> æ ‡ç­¾å’Œæœ€ç»ˆå›å¤
            thought = response.choices[0].message.reasoning_content
            content = response.choices[0].message.content
            return f"<thought>\n{thought}\n</thought>\n{content}"
        except (APIStatusError, OpenAIError) as e:
            print(f"æ¨ç†æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            time.sleep(2)
    return None

def _resume_keys(path):
    if not os.path.exists(path): return set()
    keys = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                rec = json.loads(line)
                oq = rec.get("metadata", {}).get("original_question", "")
                keys.add(hashlib.sha1(oq.encode("utf-8")).hexdigest())
            except: continue
    return keys


def process_one_item(item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    raw_q = item.get("question") or item.get("prompt")
    if not raw_q: return None
    
    t0 = time.time()
    
    # 1. é‡æ„ç—…å† (Chatæ¨¡å‹)
    reconstructed_q = call_ds_chat(f"{PROMPT_RECONSTRUCT}\nåŸå§‹æé—®ï¼š{raw_q}")
    if not reconstructed_q: return None
    
    # 2. ç”Ÿæˆ Chosen (R1 ä¸“å®¶æ¨¡å¼ - æ·±åº¦æ€è€ƒ)
    chosen_res = call_ds_reasoner(PROMPT_EXPERT, reconstructed_q, temperature=0.1)
    if not chosen_res: return None
    
    # 3. ç”Ÿæˆ Rejected (R1 å®ä¹ ç”Ÿæ¨¡å¼ - æµ…å±‚æ€è€ƒ)
    rejected_res = call_ds_reasoner(PROMPT_INTERN, reconstructed_q, temperature=1.2)
    if not rejected_res: return None
    
    return {
        "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—AIåŠ©æ‰‹ï¼Œå¿…é¡»é€šè¿‡æ·±åº¦é€»è¾‘æ¨ç†è¾…åŠ©è¯Šæ–­ã€‚",
        "history": [],
        "question": reconstructed_q,
        "response_chosen": chosen_res,
        "response_rejected": rejected_res,
        "metadata": {
            "original_question": raw_q,
            "latency": round(time.time() - t0, 2)
        }
    }


def main():
    if not api_key: raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")
    
    done_keys = _resume_keys(OUTPUT_PATH)
    items = []
    with open(INPUT_PATH, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            q = item.get("question") or item.get("prompt")
            if hashlib.sha1(q.encode("utf-8")).hexdigest() not in done_keys:
                items.append(item)

    print(f"ğŸš€ å¼€å§‹ç‚¼é‡‘ï¼å¾…å¤„ç†ï¼š{len(items)} æ¡ï¼Œå·²è·³è¿‡ï¼š{len(done_keys)} æ¡")

    with open(OUTPUT_PATH, "a", encoding="utf-8") as f_out:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_item = {executor.submit(process_one_item, it): it for it in items}
            
            for future in tqdm(as_completed(future_to_item), total=len(items), desc="æ•°æ®è’¸é¦ä¸­"):
                result = future.result()
                if result:
                    f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
                    f_out.flush()


if __name__ == "__main__":
    main()
